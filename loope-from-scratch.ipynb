{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Initialization & Session Params**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-26T18:54:02.382069Z",
     "iopub.status.busy": "2026-01-26T18:54:02.381745Z",
     "iopub.status.idle": "2026-01-26T18:54:02.390548Z",
     "shell.execute_reply": "2026-01-26T18:54:02.389672Z",
     "shell.execute_reply.started": "2026-01-26T18:54:02.382041Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "__MODEL__ = 'vit_b_16'           # 'Fourier' 'SinCos2D' 'swin', 'deit_b_16' , 'vit_b_16', 'beit_b_16', 'crossvit_base_240' 'vit_art'\n",
    "__DATASET__ = 'imagenet100'           # 'oxford_iiit', 'dtd' , 'caltech',  'cifar100' 'imagenet100'\n",
    "__ENCODER__ = 'sin_cos_1d'             # 'sin_cos_1d',  'learnable_1d', 'hilbert', 'no_pe'\n",
    "__PATCH__ = 16                    # 16, 32, 8\n",
    "__TVT_SPLIT__ = (80,10,10)\n",
    "__BATCH_SIZE_TRAIN__ = 96\n",
    "__BATCH_SIZE_VALID__ = 96\n",
    "__IMG_SIZE__ = 224\n",
    "__XIMG_SIZE__ = 256\n",
    "START_EPOCH = 1\n",
    "__EPOCHS__ = 500\n",
    "__MAX_LR__ = 0.0001\n",
    "__MIN_LR__ = 0.0001/20\n",
    "__OPTIM__ = 'adam'\n",
    "__DEVICE__ = 'gpu'\n",
    "__WARNING__ = 'supressed'\n",
    "__STEP_COUNTER__ = 50\n",
    "best_acc = 0.0\n",
    "best_epoch = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-26T18:54:02.392900Z",
     "iopub.status.busy": "2026-01-26T18:54:02.392196Z",
     "iopub.status.idle": "2026-01-26T18:54:13.387619Z",
     "shell.execute_reply": "2026-01-26T18:54:13.386973Z",
     "shell.execute_reply.started": "2026-01-26T18:54:02.392865Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-26 18:54:09.504184: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1769453649.528388     241 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1769453649.535904     241 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1769453649.555157     241 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1769453649.555186     241 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1769453649.555189     241 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1769453649.555191     241 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "if __WARNING__ == 'supressed':\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "import torchvision.models as models\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import time\n",
    "import gc\n",
    "from torchinfo import summary\n",
    "import timm\n",
    "import torch.nn.functional as F\n",
    "from transformers import ViTForImageClassification, ViTConfig\n",
    "from transformers import BeitForImageClassification\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "import albumentations as A\n",
    "import torchvision.transforms as T\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sys\n",
    "import time\n",
    "import sklearn\n",
    "import re\n",
    "import random\n",
    "from tqdm.notebook import tqdm\n",
    "import skimage.io as io\n",
    "import skimage.color as color\n",
    "from skimage.feature import local_binary_pattern\n",
    "from torch import optim \n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "import torch.utils.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-26T18:54:13.389225Z",
     "iopub.status.busy": "2026-01-26T18:54:13.388565Z",
     "iopub.status.idle": "2026-01-26T18:54:13.395188Z",
     "shell.execute_reply": "2026-01-26T18:54:13.394344Z",
     "shell.execute_reply.started": "2026-01-26T18:54:13.389196Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights_(model-vit_b_16)_(data-imagenet100)_(enc-sin_cos_1d)_(p-16)_(im-224).pth\n",
      "DEVICE WE WILL BE USING IS  cuda:0\n"
     ]
    }
   ],
   "source": [
    "BASE_PATH = f\"weights_(model-{__MODEL__})_(data-{__DATASET__})_(enc-{__ENCODER__})_(p-{__PATCH__})_(im-{__IMG_SIZE__}).pth\"\n",
    "LAST_PATH = f\"weights(last)_(model-{__MODEL__})_(data-{__DATASET__})_(enc-{__ENCODER__})_(p-{__PATCH__})_(im-{__IMG_SIZE__}).pth\"\n",
    "print(BASE_PATH)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    device = \"cuda:0\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "print(\"DEVICE WE WILL BE USING IS \", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Dataset Preparation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-26T18:54:13.397330Z",
     "iopub.status.busy": "2026-01-26T18:54:13.396858Z",
     "iopub.status.idle": "2026-01-26T18:54:13.430735Z",
     "shell.execute_reply": "2026-01-26T18:54:13.430095Z",
     "shell.execute_reply.started": "2026-01-26T18:54:13.397286Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class RandomRotate90:\n",
    "    def __init__(self, p=0.5):\n",
    "        self.p = p\n",
    "    def __call__(self, img):\n",
    "        if random.random() < self.p:\n",
    "            angle = random.choice([0, 90, 180, 270])\n",
    "            return T.functional.rotate(img, angle)\n",
    "        return img\n",
    "\n",
    "class RandomGamma:\n",
    "    def __init__(self, gamma_range=(0.8, 1.2), p=0.5):\n",
    "        self.gamma_range = gamma_range\n",
    "        self.p = p\n",
    "    def __call__(self, img):\n",
    "        if random.random() < self.p:\n",
    "            gamma = random.uniform(*self.gamma_range)\n",
    "            return T.functional.adjust_gamma(img, gamma)\n",
    "        return img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cifar-100**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-26T18:54:13.432075Z",
     "iopub.status.busy": "2026-01-26T18:54:13.431741Z",
     "iopub.status.idle": "2026-01-26T18:54:13.447213Z",
     "shell.execute_reply": "2026-01-26T18:54:13.446384Z",
     "shell.execute_reply.started": "2026-01-26T18:54:13.432038Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class cifar100(Dataset):\n",
    "    def __init__(self,files,cls_dict,mode='train'):\n",
    "        self.mode = mode\n",
    "        self.name = 'cifar_100'\n",
    "        self.cls_dict = cls_dict\n",
    "        self.num_cls = cls_dict.__len__()\n",
    "        print(files[33][2].shape)\n",
    "        self.images = [np.array(file[2]).reshape((3,32,32)).transpose((1,2,0)) for file in files]\n",
    "        self.labels = [file[1] for file in files]\n",
    "            \n",
    "        if self.mode == 'train':\n",
    "            self.transforms = A.Compose([\n",
    "                                    A.Resize(__XIMG_SIZE__,__XIMG_SIZE__),\n",
    "                                    A.HorizontalFlip(p=0.5),\n",
    "                                    A.Rotate(limit=15, p=0.5),\n",
    "                                    A.RandomCrop(__IMG_SIZE__, __IMG_SIZE__),\n",
    "                                    ToTensorV2()\n",
    "                                    ])\n",
    "        else :\n",
    "            self.transforms = A.Compose([\n",
    "                                    A.Resize(__IMG_SIZE__, __IMG_SIZE__),\n",
    "                                    ToTensorV2()\n",
    "                                ])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    def __getitem__(self,idx):\n",
    "        image = self.images[idx]\n",
    "        label = self.labels[idx]\n",
    "        image = self.transforms(image=image)['image']/255.00\n",
    "        temp = torch.zeros(self.num_cls).float()\n",
    "        temp[label] = 1.0\n",
    "        \n",
    "        return image, temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-26T18:54:13.448647Z",
     "iopub.status.busy": "2026-01-26T18:54:13.448315Z",
     "iopub.status.idle": "2026-01-26T18:54:13.465762Z",
     "shell.execute_reply": "2026-01-26T18:54:13.464926Z",
     "shell.execute_reply.started": "2026-01-26T18:54:13.448623Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "if __DATASET__ == 'cifar100' or __DATASET__ == 'pretest':\n",
    "    try:\n",
    "        split_dir = \"/kaggle/input/cifar100-splits/cifar100/\"\n",
    "        if not os.path.isdir(split_dir):\n",
    "            raise FileNotFoundError(f\"Directory '{directory}' does not exist!\")\n",
    "        train_files = np.load(split_dir+\"train_files.npz\", allow_pickle=True)[\"data\"]\n",
    "        valid_files = np.load(split_dir+\"valid_files.npz\", allow_pickle=True)[\"data\"]\n",
    "        test_files = np.load(split_dir+\"test_files.npz\", allow_pickle=True)[\"data\"]\n",
    "        print('Successfully preloaded train, valid, test splits.....')\n",
    "    except:\n",
    "        def unpickle(file):\n",
    "            import pickle\n",
    "            with open(file, 'rb') as fo:\n",
    "                dict = pickle.load(fo, encoding='bytes')\n",
    "            return dict\n",
    "        train_dir = '/kaggle/input/cifar100/train'\n",
    "        train_dict = unpickle(train_dir)\n",
    "        lst = list(train_dict.keys())\n",
    "        length = train_dict[lst[0]].__len__()\n",
    "        files = [(train_dict[lst[0]][i], train_dict[lst[2]][i], train_dict[lst[4]][i]) for i in range(length)]\n",
    "        train_files, vt_files = train_test_split(files, test_size = (__TVT_SPLIT__[1]+__TVT_SPLIT__[2])/100, random_state=42)\n",
    "        valid_files, test_files = train_test_split(vt_files, test_size = (__TVT_SPLIT__[2])/(__TVT_SPLIT__[1]+__TVT_SPLIT__[2]), random_state=42)\n",
    "        try:\n",
    "            os.mkdir(\"/kaggle/working/cifar100\")\n",
    "        except:\n",
    "            print(\"sub-folder already created\")\n",
    "        np.savez('/kaggle/working/cifar100/train_files.npz', data=np.array(train_files,dtype=object))\n",
    "        np.savez('/kaggle/working/cifar100/valid_files.npz', data=np.array(valid_files,dtype=object))\n",
    "        np.savez('/kaggle/working/cifar100/test_files.npz', data=np.array(test_files,dtype=object))\n",
    "    print(\"total classes: \", 100)\n",
    "    classes = [i for i in range(100)]\n",
    "    train_dataset = cifar100(train_files, classes, 'train')\n",
    "    valid_dataset = cifar100(valid_files, classes, 'valid')\n",
    "    test_dataset = cifar100(test_files, classes, 'test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ImageNeT100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-26T18:54:13.467102Z",
     "iopub.status.busy": "2026-01-26T18:54:13.466826Z",
     "iopub.status.idle": "2026-01-26T18:54:13.866176Z",
     "shell.execute_reply": "2026-01-26T18:54:13.865368Z",
     "shell.execute_reply.started": "2026-01-26T18:54:13.467077Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# =========================\n",
    "# ImageNet100 drop-in setup\n",
    "# =========================\n",
    "\n",
    "import os, glob\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import cv2\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "\n",
    "# ---- (recommended) for ViT scratch training on ImageNet-like data\n",
    "# Keep your existing __IMG_SIZE__ / __XIMG_SIZE__ if already defined.\n",
    "# Typical: __IMG_SIZE__=224, __XIMG_SIZE__=256\n",
    "IMAGENET_MEAN = (0.485, 0.456, 0.406)\n",
    "IMAGENET_STD  = (0.229, 0.224, 0.225)\n",
    "\n",
    "# IMPORTANT:\n",
    "# Your existing pipeline does: transforms(...)->ToTensorV2() then \"/255.0\" in __getitem__.\n",
    "# Albumentations Normalize would become wrong if you keep \"/255.0\".\n",
    "# So we normalize using a Lambda that is compatible with the later \"/255.0\":\n",
    "#   output = (x - mean*255)/std   and later /255 => (x/255 - mean)/std  (correct)\n",
    "def _imagenet_norm_keep_div255(img, **kwargs):\n",
    "    img = img.astype(np.float32)\n",
    "    mean = np.array(IMAGENET_MEAN, dtype=np.float32) * 255.0\n",
    "    std  = np.array(IMAGENET_STD,  dtype=np.float32)\n",
    "    return (img - mean) / std\n",
    "\n",
    "class imagenet100(Dataset):\n",
    "    def __init__(self, files, cls_dict, mode='train', preload_ram=True):\n",
    "        self.mode = mode\n",
    "        self.name = 'imagenet_100'\n",
    "        self.cls_dict = cls_dict\n",
    "        self.num_cls = cls_dict.__len__()\n",
    "\n",
    "        self.labels = [int(file[1]) for file in files]\n",
    "        self.preload_ram = preload_ram\n",
    "\n",
    "        if self.preload_ram:\n",
    "            # Preload into a single contiguous uint8 array to minimize overhead\n",
    "            N = len(files)\n",
    "            self.images = np.empty((N, __IMG_SIZE__, __IMG_SIZE__, 3), dtype=np.uint8)\n",
    "\n",
    "            for i, file in enumerate(files):\n",
    "                p = file[2]\n",
    "                img = cv2.imread(p, cv2.IMREAD_COLOR)\n",
    "                if img is None:\n",
    "                    raise FileNotFoundError(f\"Failed to read image: {p}\")\n",
    "                img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)  # uint8\n",
    "\n",
    "                # Safety: if any image isn't exactly 224x224, resize once during preload\n",
    "                if img.shape[0] != __IMG_SIZE__ or img.shape[1] != __IMG_SIZE__:\n",
    "                    img = cv2.resize(img, (__IMG_SIZE__, __IMG_SIZE__), interpolation=cv2.INTER_AREA)\n",
    "\n",
    "                self.images[i] = img\n",
    "        else:\n",
    "            self.images = [file[2] for file in files]  # paths\n",
    "\n",
    "        # transforms unchanged\n",
    "        if self.mode == 'train':\n",
    "            self.transforms = A.Compose([\n",
    "                A.RandomResizedCrop(\n",
    "                    size=(__IMG_SIZE__, __IMG_SIZE__),\n",
    "                    scale=(0.08, 1.0),\n",
    "                    ratio=(0.75, 1.3333333),\n",
    "                    interpolation=cv2.INTER_CUBIC,\n",
    "                    p=1.0\n",
    "                ),\n",
    "                A.HorizontalFlip(p=0.5),\n",
    "                A.OneOf([\n",
    "                    A.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3, hue=0.1, p=1.0),\n",
    "                    A.RandomBrightnessContrast(brightness_limit=0.25, contrast_limit=0.25, p=1.0),\n",
    "                    A.HueSaturationValue(hue_shift_limit=10, sat_shift_limit=15, val_shift_limit=10, p=1.0),\n",
    "                    A.RGBShift(r_shift_limit=10, g_shift_limit=10, b_shift_limit=10, p=1.0),\n",
    "                ], p=0.8),\n",
    "                A.OneOf([\n",
    "                    A.GaussNoise(var_limit=(10.0, 50.0), p=1.0),\n",
    "                    A.GaussianBlur(blur_limit=(3, 5), p=1.0),\n",
    "                    A.MotionBlur(blur_limit=(3, 5), p=1.0),\n",
    "                ], p=0.2),\n",
    "                A.CoarseDropout(\n",
    "                    max_holes=1,\n",
    "                    max_height=int(__IMG_SIZE__ * 0.25),\n",
    "                    max_width=int(__IMG_SIZE__ * 0.25),\n",
    "                    min_holes=1,\n",
    "                    min_height=int(__IMG_SIZE__ * 0.10),\n",
    "                    min_width=int(__IMG_SIZE__ * 0.10),\n",
    "                    fill_value=0,\n",
    "                    p=0.25\n",
    "                ),\n",
    "                A.Lambda(image=_imagenet_norm_keep_div255),\n",
    "                ToTensorV2(),\n",
    "            ])\n",
    "        else:\n",
    "            self.transforms = A.Compose([\n",
    "                A.Resize(__XIMG_SIZE__, __XIMG_SIZE__, interpolation=cv2.INTER_CUBIC),\n",
    "                A.CenterCrop(__IMG_SIZE__, __IMG_SIZE__),\n",
    "                A.Lambda(image=_imagenet_norm_keep_div255),\n",
    "                ToTensorV2(),\n",
    "            ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.preload_ram:\n",
    "            img = self.images[idx]  # uint8 RGB\n",
    "        else:\n",
    "            path = self.images[idx]\n",
    "            img = cv2.imread(path, cv2.IMREAD_COLOR)\n",
    "            if img is None:\n",
    "                raise FileNotFoundError(f\"Failed to read image: {path}\")\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        img = self.transforms(image=img)['image'] / 255.0\n",
    "\n",
    "        label = self.labels[idx]\n",
    "        temp = torch.zeros(self.num_cls).float()\n",
    "        temp[label] = 1.0\n",
    "        return img, temp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-26T18:54:13.867828Z",
     "iopub.status.busy": "2026-01-26T18:54:13.867500Z",
     "iopub.status.idle": "2026-01-26T18:59:11.912161Z",
     "shell.execute_reply": "2026-01-26T18:59:11.911420Z",
     "shell.execute_reply.started": "2026-01-26T18:54:13.867795Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully preloaded train/valid/test splits from: /kaggle/working/imagenet100\n",
      "total classes: 100\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Split handling (drop-in)\n",
    "# =========================\n",
    "\n",
    "if __DATASET__ == 'imagenet100' or __DATASET__ == 'pretest':\n",
    "    root_dir = \"/kaggle/input/processed-imagenet-dataset-224\"   # class folders live here\n",
    "    split_dir = \"/kaggle/working/imagenet100\"\n",
    "    os.makedirs(split_dir, exist_ok=True)\n",
    "\n",
    "    def _load_splits(dirpath):\n",
    "        tr = np.load(os.path.join(dirpath, \"train_files.npz\"), allow_pickle=True)[\"data\"]\n",
    "        va = np.load(os.path.join(dirpath, \"valid_files.npz\"), allow_pickle=True)[\"data\"]\n",
    "        te = np.load(os.path.join(dirpath, \"test_files.npz\"),  allow_pickle=True)[\"data\"]\n",
    "        return tr, va, te\n",
    "\n",
    "    # Try load cached splits (first from an input dataset if you have one, else from working)\n",
    "    loaded = False\n",
    "    for cand in [\n",
    "        \"/kaggle/input/imagenet100-splits/imagenet100\",  # optional (if you created/uploaded)\n",
    "        split_dir\n",
    "    ]:\n",
    "        try:\n",
    "            if os.path.isfile(os.path.join(cand, \"train_files.npz\")):\n",
    "                train_files, valid_files, test_files = _load_splits(cand)\n",
    "                print(f\"Successfully preloaded train/valid/test splits from: {cand}\")\n",
    "                loaded = True\n",
    "                break\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    if not loaded:\n",
    "        # discover class folders (e.g., n01440764)\n",
    "        classes_synset = sorted([\n",
    "            d for d in os.listdir(root_dir)\n",
    "            if os.path.isdir(os.path.join(root_dir, d)) and not d.startswith(\".\")\n",
    "        ])\n",
    "        cls2idx = {c: i for i, c in enumerate(classes_synset)}\n",
    "\n",
    "        # build file tuples: (placeholder, label_index, image_path)\n",
    "        files = []\n",
    "        exts = (\"*.JPEG\", \"*.JPG\", \"*.jpeg\", \"*.jpg\", \"*.png\")\n",
    "        for c in classes_synset:\n",
    "            cdir = os.path.join(root_dir, c)\n",
    "            for ext in exts:\n",
    "                for p in glob.glob(os.path.join(cdir, ext)):\n",
    "                    files.append((None, cls2idx[c], p))\n",
    "\n",
    "        # stratified TVT split like your CIFAR code\n",
    "        labels = [f[1] for f in files]\n",
    "        vt_ratio = (__TVT_SPLIT__[1] + __TVT_SPLIT__[2]) / 100.0\n",
    "\n",
    "        train_files, vt_files = train_test_split(\n",
    "            files,\n",
    "            test_size=vt_ratio,\n",
    "            random_state=42,\n",
    "            stratify=labels\n",
    "        )\n",
    "\n",
    "        vt_labels = [f[1] for f in vt_files]\n",
    "        test_ratio_inside_vt = (__TVT_SPLIT__[2]) / (__TVT_SPLIT__[1] + __TVT_SPLIT__[2])\n",
    "\n",
    "        valid_files, test_files = train_test_split(\n",
    "            vt_files,\n",
    "            test_size=test_ratio_inside_vt,\n",
    "            random_state=42,\n",
    "            stratify=vt_labels\n",
    "        )\n",
    "\n",
    "        # cache\n",
    "        np.savez(os.path.join(split_dir, \"train_files.npz\"), data=np.array(train_files, dtype=object))\n",
    "        np.savez(os.path.join(split_dir, \"valid_files.npz\"), data=np.array(valid_files, dtype=object))\n",
    "        np.savez(os.path.join(split_dir, \"test_files.npz\"),  data=np.array(test_files,  dtype=object))\n",
    "\n",
    "        print(f\"Created and cached splits in: {split_dir}\")\n",
    "        print(\"Classes discovered:\", len(classes_synset))\n",
    "\n",
    "    # Your original code passes \"classes\" only to define num_cls.\n",
    "    # Since labels are already integer indices 0..C-1, we pass range(C).\n",
    "    # If you loaded cached splits, infer C from labels.\n",
    "    all_labels = np.array([f[1] for f in train_files], dtype=np.int64)\n",
    "    num_classes = int(all_labels.max()) + 1\n",
    "    print(\"total classes:\", num_classes)\n",
    "\n",
    "    classes = [i for i in range(num_classes)]\n",
    "    train_dataset = imagenet100(train_files, classes, 'train')\n",
    "    valid_dataset = imagenet100(valid_files, classes, 'valid')\n",
    "    test_dataset  = imagenet100(test_files,  classes, 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-26T18:59:11.913488Z",
     "iopub.status.busy": "2026-01-26T18:59:11.913248Z",
     "iopub.status.idle": "2026-01-26T18:59:11.918597Z",
     "shell.execute_reply": "2026-01-26T18:59:11.917900Z",
     "shell.execute_reply.started": "2026-01-26T18:59:11.913463Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATASET LOADED NAMED: imagenet_100 WITH NUM OF CLASS: 100\n"
     ]
    }
   ],
   "source": [
    "__NUM_CLASS__ = train_dataset.num_cls\n",
    "print(f\"DATASET LOADED NAMED: {train_dataset.name} WITH NUM OF CLASS: {__NUM_CLASS__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Positional Encoders**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**No PE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-26T18:59:11.921540Z",
     "iopub.status.busy": "2026-01-26T18:59:11.921278Z",
     "iopub.status.idle": "2026-01-26T18:59:11.933957Z",
     "shell.execute_reply": "2026-01-26T18:59:11.933160Z",
     "shell.execute_reply.started": "2026-01-26T18:59:11.921517Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def get_zero_pos_embed(embed_dim, seq_length):\n",
    "    return nn.Parameter(torch.zeros((1, seq_length, embed_dim), dtype=torch.float32), requires_grad=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sin-Cos 1D PE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-26T18:59:11.935393Z",
     "iopub.status.busy": "2026-01-26T18:59:11.935006Z",
     "iopub.status.idle": "2026-01-26T18:59:11.949166Z",
     "shell.execute_reply": "2026-01-26T18:59:11.948344Z",
     "shell.execute_reply.started": "2026-01-26T18:59:11.935357Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def get_1d_sincos_pos_embed(embed_dim, seq_length):\n",
    "    position = torch.arange(seq_length, dtype=torch.float32).unsqueeze(1)  # (seq_length, 1)\n",
    "    div_term = torch.exp(torch.arange(0, embed_dim, 2, dtype=torch.float32) * -(np.log(10000.0) / embed_dim))\n",
    "    pos_embed = torch.zeros((seq_length, embed_dim), dtype=torch.float32)\n",
    "    pos_embed[:, 0::2] = torch.sin(position * div_term)  # apply sin to even indices in the embedding\n",
    "    pos_embed[:, 1::2] = torch.cos(position * div_term)  # apply cos to odd indices in the embedding\n",
    "    return nn.Parameter(pos_embed.unsqueeze(0),requires_grad=False)  # add a batch dimension so shape becomes (1, seq_length, embed_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1D learnable PE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-26T18:59:11.950619Z",
     "iopub.status.busy": "2026-01-26T18:59:11.950306Z",
     "iopub.status.idle": "2026-01-26T18:59:11.967437Z",
     "shell.execute_reply": "2026-01-26T18:59:11.966716Z",
     "shell.execute_reply.started": "2026-01-26T18:59:11.950584Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def get_1d_learnable_pos_embed(embed_dim, seq_length):\n",
    "     return nn.Parameter(torch.empty(1, seq_length, embed_dim).normal_(std=0.02), requires_grad=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hilbert PE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-26T18:59:11.968556Z",
     "iopub.status.busy": "2026-01-26T18:59:11.968306Z",
     "iopub.status.idle": "2026-01-26T18:59:11.983408Z",
     "shell.execute_reply": "2026-01-26T18:59:11.982789Z",
     "shell.execute_reply.started": "2026-01-26T18:59:11.968534Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def gilbert_2d(width, height):\n",
    "    def sgn(x):\n",
    "        return 1 if x > 0 else -1 if x < 0 else 0\n",
    "    \n",
    "    def generate(x, y, ax, ay, bx, by):\n",
    "        w = abs(ax + ay)\n",
    "        h = abs(bx + by)\n",
    "        dax, day = sgn(ax), sgn(ay)\n",
    "        dbx, dby = sgn(bx), sgn(by)\n",
    "\n",
    "        if h == 1:\n",
    "            return [(x + dax*i, y + day*i) for i in range(w)]\n",
    "        if w == 1:\n",
    "            return [(x + dbx*i, y + dby*i) for i in range(h)]\n",
    "\n",
    "        ax2, ay2 = ax//2, ay//2\n",
    "        bx2, by2 = bx//2, by//2\n",
    "        w2, h2 = abs(ax2 + ay2), abs(bx2 + by2)\n",
    "\n",
    "        if 2*w > 3*h:\n",
    "            if w2%2 and w>2:\n",
    "                ax2 += dax\n",
    "                ay2 += day\n",
    "            return generate(x, y, ax2, ay2, bx, by) + \\\n",
    "                   generate(x+ax2, y+ay2, ax-ax2, ay-ay2, bx, by)\n",
    "        else:\n",
    "            if h2%2 and h>2:\n",
    "                bx2 += dbx\n",
    "                by2 += dby\n",
    "            return generate(x, y, bx2, by2, ax2, ay2) + \\\n",
    "                   generate(x+bx2, y+by2, ax, ay, bx-bx2, by-by2) + \\\n",
    "                   generate(x+(ax-dax)+(bx2-dbx), \n",
    "                            y+(ay-day)+(by2-dby), \n",
    "                            -bx2, -by2, -ax+ax2, -ay+ay2)\n",
    "\n",
    "    # Generate coordinates and create index array\n",
    "    curve = generate(0, 0, width, 0, 0, height)\n",
    "    arr = np.zeros((height, width), dtype=int)\n",
    "    for idx, (x, y) in enumerate(curve):\n",
    "        arr[y, x] = idx+1  # Note y is first dimension in numpy arrays\n",
    "    \n",
    "    return arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-26T18:59:11.984603Z",
     "iopub.status.busy": "2026-01-26T18:59:11.984261Z",
     "iopub.status.idle": "2026-01-26T18:59:12.003959Z",
     "shell.execute_reply": "2026-01-26T18:59:12.003394Z",
     "shell.execute_reply.started": "2026-01-26T18:59:11.984579Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def generate_hilbert_grid(N):\n",
    "    def rot(n, x, y, rx, ry):\n",
    "        if ry == 0:\n",
    "            if rx == 1:\n",
    "                x, y = n - 1 - x, n - 1 - y\n",
    "            x, y = y, x\n",
    "        return x, y\n",
    "\n",
    "    def hilbert_index(n, d):\n",
    "        x = y = 0\n",
    "        t = d\n",
    "        s = 1\n",
    "        while s < n:\n",
    "            rx = (t // 2) & 1\n",
    "            ry = (t ^ rx) & 1\n",
    "            x, y = rot(s, x, y, rx, ry)\n",
    "            x += s * rx\n",
    "            y += s * ry\n",
    "            t //= 4\n",
    "            s *= 2\n",
    "        return x, y\n",
    "\n",
    "    grid = [[-1 for _ in range(N)] for _ in range(N)]\n",
    "    for d in range(N * N):\n",
    "        x, y = hilbert_index(N, d)\n",
    "        grid[y][x] = d\n",
    "    return grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-26T18:59:12.005433Z",
     "iopub.status.busy": "2026-01-26T18:59:12.004972Z",
     "iopub.status.idle": "2026-01-26T18:59:12.023577Z",
     "shell.execute_reply": "2026-01-26T18:59:12.022856Z",
     "shell.execute_reply.started": "2026-01-26T18:59:12.005409Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def get_hilbert_pos_embed(embed_dim, seq_length):\n",
    "    N = int(np.sqrt(seq_length-1))\n",
    "    grid = gilbert_2d(N, N)\n",
    "    grid = torch.Tensor(grid).reshape(seq_length-1)\n",
    "    position = torch.cat((torch.tensor([0]), grid)).unsqueeze(1)\n",
    "    div_term = torch.exp(torch.arange(0, embed_dim, 2, dtype=torch.float32) * -(np.log(10000.0) / embed_dim))\n",
    "    pos_embed = torch.zeros((seq_length, embed_dim), dtype=torch.float32)\n",
    "    pos_embed[:, 0::2] = torch.sin(position * div_term)  # apply sin to even indices in the embedding\n",
    "    pos_embed[:, 1::2] = torch.cos(position * div_term)  # apply cos to odd indices in the embedding\n",
    "    return nn.Parameter(pos_embed.unsqueeze(0),requires_grad=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PE Callers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-26T18:59:12.025195Z",
     "iopub.status.busy": "2026-01-26T18:59:12.024830Z",
     "iopub.status.idle": "2026-01-26T18:59:12.037767Z",
     "shell.execute_reply": "2026-01-26T18:59:12.037099Z",
     "shell.execute_reply.started": "2026-01-26T18:59:12.025146Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "if __ENCODER__ == 'no_pe':\n",
    "    pe_caller = get_zero_pos_embed\n",
    "elif __ENCODER__ == 'sin_cos_1d' or __ENCODER__ == 'pretest':\n",
    "    pe_caller = get_1d_sincos_pos_embed\n",
    "elif __ENCODER__ == 'learnable_1d':\n",
    "    pe_caller = get_1d_learnable_pos_embed\n",
    "elif __ENCODER__ == 'hilbert':\n",
    "    pe_caller = get_hilbert_pos_embed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **MODEL**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-26T18:59:12.039298Z",
     "iopub.status.busy": "2026-01-26T18:59:12.038848Z",
     "iopub.status.idle": "2026-01-26T18:59:12.054827Z",
     "shell.execute_reply": "2026-01-26T18:59:12.053960Z",
     "shell.execute_reply.started": "2026-01-26T18:59:12.039275Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class art_PE(nn.Module):\n",
    "    def __init__(self, seq_length, hidden_dim, img_size=224, patch_size=16):\n",
    "        super().__init__()\n",
    "        self.seq_length, self.hidden_dim, self.img_size, self.patch_size = seq_length, hidden_dim, img_size, patch_size\n",
    "        self.register_buffer('cord', torch.stack(torch.meshgrid(torch.linspace(-1,1,img_size, dtype=torch.float), torch.linspace(-1,1,img_size, dtype=torch.float), indexing='ij')))\n",
    "        self.art_conv = nn.Sequential(\n",
    "            nn.Conv2d(5, 32, 16, 16, 0), nn.GELU(),\n",
    "            nn.Conv2d(32, 16, 5, 1, 2), nn.GELU(),\n",
    "            nn.Conv2d(16, 8, 5, 1, 2), nn.GELU(),\n",
    "            nn.Conv2d(8, 4, 5, 1, 2), nn.GELU(),\n",
    "            nn.Conv2d(4, 1, 5, 1, 2), nn.GELU(),\n",
    "            nn.BatchNorm2d(1), nn.Flatten(),\n",
    "            nn.Linear((img_size // patch_size) ** 2, self.seq_length-1, bias=False), nn.Sigmoid()\n",
    "        )\n",
    "        #self.wrap_pos = nn.Parameter(torch.zeros(seq_length, 1, dtype=torch.float32))\n",
    "        self.register_buffer('wrap_pos_hilbert', torch.Tensor(gilbert_2d((img_size // patch_size), (img_size // patch_size))).reshape(self.seq_length-1).unsqueeze(1))\n",
    "        self.register_buffer('div_term', torch.exp(torch.arange(0, hidden_dim, 2, dtype=torch.float32).unsqueeze(0) * -(np.log(10000.0) / hidden_dim)))\n",
    "        #self.div_term = nn.Parameter(torch.randn(1,self.hidden_dim//2))\n",
    "        self.cls_token = nn.Parameter(torch.zeros((1,self.hidden_dim)))\n",
    "        #self.fc1 = nn.Linear(self.hidden_dim, 32, bias=True)\n",
    "        #self.fc2 = nn.Linear(32, self.hidden_dim, bias=True)\n",
    "        #self.activation = nn.GELU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        n = x.shape[0]\n",
    "        out = self.art_conv(torch.cat([x, self.cord.unsqueeze(0).expand(n, -1, -1, -1)], dim=1)).unsqueeze(2)\n",
    "        out = 2*out-1\n",
    "        out = out + self.wrap_pos_hilbert\n",
    "        #out = self.wrap_pos_hilbert.unsqueeze(0).expand(n,-1,-1)\n",
    "        sin_part, cos_part = torch.sin(out * self.div_term), torch.cos(out * self.div_term)\n",
    "        pos_embedding = torch.empty((n, self.seq_length, self.hidden_dim), dtype=self.wrap_pos_hilbert.dtype, device=x.device)\n",
    "        pos_embedding[:, 1:, 0::2], pos_embedding[:, 1:, 1::2] = sin_part, cos_part\n",
    "        pos_embedding[:, :1, :] = self.cls_token.unsqueeze(0).expand(n,-1,-1)\n",
    "        #pos_embedding = self.fc2(self.activation(self.fc1(pos_embedding)))\n",
    "        return pos_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-26T18:59:12.056173Z",
     "iopub.status.busy": "2026-01-26T18:59:12.055842Z",
     "iopub.status.idle": "2026-01-26T18:59:12.075618Z",
     "shell.execute_reply": "2026-01-26T18:59:12.074842Z",
     "shell.execute_reply.started": "2026-01-26T18:59:12.056141Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class ViT_Art(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = ViTForImageClassification.from_pretrained('google/vit-base-patch16-224')\n",
    "        _, self.seq_length, self.hidden_dim = self.model.vit.embeddings.position_embeddings.shape\n",
    "        self.model.vit.embeddings.position_embeddings = get_zero_pos_embed(self.hidden_dim, self.seq_length)\n",
    "        self.pos_embedding = art_PE(self.seq_length, self.hidden_dim)\n",
    "        self.patch = self.model.vit.embeddings.patch_embeddings\n",
    "        self.encoder = self.model.vit.encoder\n",
    "        self.layernorm = self.model.vit.layernorm\n",
    "        self.classifier = nn.Linear(in_features=self.hidden_dim, out_features=__NUM_CLASS__, bias=True)\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, self.hidden_dim))\n",
    "    def forward(self, x):\n",
    "        n = x.shape[0]\n",
    "        patched = self.patch(x)\n",
    "        batched_cls_token = self.cls_token.expand(n, -1, -1)\n",
    "        patched = torch.cat([batched_cls_token, patched], dim=1)\n",
    "        patched = patched + self.pos_embedding(x)\n",
    "        patched = self.encoder(patched)\n",
    "        patched = self.layernorm(patched.last_hidden_state)\n",
    "        patched = patched[:, 0]\n",
    "        patched = self.classifier(patched)\n",
    "        return patched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-26T18:59:12.076656Z",
     "iopub.status.busy": "2026-01-26T18:59:12.076389Z",
     "iopub.status.idle": "2026-01-26T18:59:12.095076Z",
     "shell.execute_reply": "2026-01-26T18:59:12.094363Z",
     "shell.execute_reply.started": "2026-01-26T18:59:12.076632Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "if __MODEL__ == 'vit_art' or __MODEL__ == 'pretest':\n",
    "    model = ViT_Art()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fourier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-26T18:59:12.096602Z",
     "iopub.status.busy": "2026-01-26T18:59:12.096124Z",
     "iopub.status.idle": "2026-01-26T18:59:12.115432Z",
     "shell.execute_reply": "2026-01-26T18:59:12.114702Z",
     "shell.execute_reply.started": "2026-01-26T18:59:12.096571Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "if __MODEL__ == 'Fourier' or __MODEL__ == 'pretest':    \n",
    "    class FourierPositionalEncoding(nn.Module):\n",
    "        def __init__(self, num_patches, dim, fourier_dim=768, hidden_dim=32, groups=1):\n",
    "            super().__init__()\n",
    "            self.num_patches = num_patches  # N = 196\n",
    "            self.fourier_dim = fourier_dim  # |F| = 768\n",
    "            self.hidden_dim = hidden_dim    # |H| = 32\n",
    "            self.dim = dim                  # D = 768\n",
    "            self.groups = groups            # G = 1\n",
    "            self.M = 2                      # M = 2D positional values\n",
    "\n",
    "            grid_size = int(np.sqrt(self.num_patches))  # Typically 14x14 for 196 patches\n",
    "            x = np.linspace(-1, 1, grid_size)\n",
    "            y = np.linspace(-1, 1, grid_size)\n",
    "            xx, yy = np.meshgrid(x, y)\n",
    "            positions = np.stack([xx.flatten(), yy.flatten()], axis=-1)  # (num_patches, 2)\n",
    "            #print(positions.shape)\n",
    "            \n",
    "            # Reshape to (N, G, M) = (196, 3, 2)\n",
    "            self.register_buffer('positions', torch.tensor(positions, dtype=torch.float32).unsqueeze(1))  # (196, 1, 2)\n",
    "            # Learnable Fourier weights Wr ∈ R^(|F|/2, M), sampled from N(0, γ^-2)\n",
    "            self.Wr = nn.Parameter(torch.randn(fourier_dim//2, self.M))  # (384, 2)\n",
    "            \n",
    "            # MLP layers using Linear instead of manual weight parameters\n",
    "            self.fc1 = nn.Linear(fourier_dim, hidden_dim, bias=True)  # (768, 32)\n",
    "            self.fc2 = nn.Linear(hidden_dim, dim // groups, bias=True)  # (32, 256)\n",
    "            self.activation = nn.GELU()\n",
    "            self.cls_embed = nn.Parameter(torch.zeros((1, self.dim)))\n",
    "\n",
    "        def forward(self, x):\n",
    "            \"\"\"\n",
    "            Compute Fourier-based positional encoding.\n",
    "            \"\"\"    \n",
    "            # Compute Fourier features F = [cos(XWr^T); sin(XWr^T)]\n",
    "            proj = torch.matmul(self.positions, self.Wr.T)  # (196, 3, 384)\n",
    "            #F = torch.cat([torch.cos(proj), torch.sin(proj)], dim=-1)  # (196, 3, 768)\n",
    "            F = torch.empty((self.num_patches,self.groups, self.fourier_dim), dtype=proj.dtype, device=x.device)\n",
    "            F[:,:,0::2] = torch.sin(proj)\n",
    "            F[:,:,1::2] = torch.cos(proj)\n",
    "            \n",
    "    \n",
    "            # Pass through MLP: Y = GeLU(FW1 + B1)W2 + B2\n",
    "            Y = self.fc2(self.activation(self.fc1(F)))  # (196, 3, 256)\n",
    "    \n",
    "            # Reshape Y to (N, D) = (196, 768)\n",
    "            PEX = Y.reshape(self.num_patches, self.dim)\n",
    "            pos = torch.cat([self.cls_embed, PEX], dim=0)\n",
    "    \n",
    "            return pos+x\n",
    "    class ViT_Fourier(nn.Module):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "            self.model = ViTForImageClassification.from_pretrained('google/vit-base-patch16-224')\n",
    "            _, self.seq_length, self.hidden_dim = self.model.vit.embeddings.position_embeddings.shape\n",
    "            self.model.vit.embeddings.position_embeddings = get_zero_pos_embed(self.hidden_dim, self.seq_length)\n",
    "            num_patches = self.model.vit.embeddings.patch_embeddings.num_patches  # Should be 196 for 224x224 images with 16x16 patches\n",
    "            dim = self.model.config.hidden_size  # Should be 768\n",
    "            # Compute Fourier positional encoding\n",
    "            self.pos_embedding = FourierPositionalEncoding(num_patches=num_patches, dim=dim, fourier_dim=768, hidden_dim=32, groups=1)\n",
    "            self.patch = self.model.vit.embeddings.patch_embeddings\n",
    "            self.encoder = self.model.vit.encoder\n",
    "            self.layernorm = self.model.vit.layernorm\n",
    "            self.classifier = nn.Linear(in_features=self.hidden_dim, out_features=__NUM_CLASS__, bias=True)\n",
    "            self.cls_token = nn.Parameter(torch.zeros(1, 1, self.hidden_dim))\n",
    "        def forward(self, x):\n",
    "            n = x.shape[0]\n",
    "            patched = self.patch(x)\n",
    "            batched_cls_token = self.cls_token.expand(n, -1, -1)\n",
    "            patched = torch.cat([batched_cls_token, patched], dim=1)\n",
    "            patched = self.pos_embedding(patched)\n",
    "            patched = self.encoder(patched)\n",
    "            patched = self.layernorm(patched.last_hidden_state)\n",
    "            patched = patched[:, 0]\n",
    "            patched = self.classifier(patched)\n",
    "            return patched\n",
    "\n",
    "    model = ViT_Fourier()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SinCos2D**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-26T18:59:12.116467Z",
     "iopub.status.busy": "2026-01-26T18:59:12.116217Z",
     "iopub.status.idle": "2026-01-26T18:59:12.139224Z",
     "shell.execute_reply": "2026-01-26T18:59:12.138488Z",
     "shell.execute_reply.started": "2026-01-26T18:59:12.116443Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "if __MODEL__ == 'SinCos2D' or __MODEL__ == 'pretest':    \n",
    "    class Sin2DPositionalEncoding(nn.Module):\n",
    "        def __init__(self, num_patches, dim):\n",
    "            super().__init__()\n",
    "            self.num_patches = num_patches\n",
    "            self.dim = dim\n",
    "            # Frequency scaling term (same as in Transformers)\n",
    "            self.register_buffer('div_term', torch.exp(torch.arange(0, dim, 2).float() * (-np.log(10000.0) / dim)))\n",
    "            grid_size = int(np.sqrt(self.num_patches))  # Typically 14x14 for 196 patches\n",
    "            x = np.linspace(-1, 1, grid_size)\n",
    "            y = np.linspace(-1, 1, grid_size)\n",
    "            xx, yy = np.meshgrid(x, y)\n",
    "            positions = np.stack([xx.flatten(), yy.flatten()], axis=-1)  # (num_patches, 2)\n",
    "            self.register_buffer('normalized_positions', torch.tensor(positions, dtype=torch.float32))\n",
    "            self.register_buffer('cls_embed', torch.zeros((1, self.dim)))\n",
    "        def forward(self, x):\n",
    "            positions = self.normalized_positions.unsqueeze(-1)  # (196, 2, 1)\n",
    "            pe = torch.zeros(self.num_patches, self.dim, device=x.device)  # (196, d)\n",
    "            pe[:, 0::2] = torch.sin(positions[:, 0] * self.div_term[: self.dim // 2]) + torch.sin(positions[:, 1] * self.div_term[: self.dim // 2])\n",
    "            pe[:, 1::2] = torch.cos(positions[:, 0] * self.div_term[: self.dim // 2]) + torch.cos(positions[:, 1] * self.div_term[: self.dim // 2])\n",
    "            pe = torch.cat([self.cls_embed, pe], dim=0).unsqueeze(0)\n",
    "            return pe+x  # Shape: (196, d)\n",
    "    class ViT_SinCos2D(nn.Module):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "            self.model = ViTForImageClassification.from_pretrained('google/vit-base-patch16-224')\n",
    "            _, self.seq_length, self.hidden_dim = self.model.vit.embeddings.position_embeddings.shape\n",
    "            self.model.vit.embeddings.position_embeddings = get_zero_pos_embed(self.hidden_dim, self.seq_length)\n",
    "            num_patches = self.model.vit.embeddings.patch_embeddings.num_patches  # Should be 196 for 224x224 images with 16x16 patches\n",
    "            dim = self.model.config.hidden_size  # Should be 768\n",
    "            # Compute Fourier positional encoding\n",
    "            self.pos_embedding = Sin2DPositionalEncoding(num_patches=num_patches, dim=dim)\n",
    "            self.patch = self.model.vit.embeddings.patch_embeddings\n",
    "            self.encoder = self.model.vit.encoder\n",
    "            self.layernorm = self.model.vit.layernorm\n",
    "            self.classifier = nn.Linear(in_features=self.hidden_dim, out_features=__NUM_CLASS__, bias=True)\n",
    "            self.cls_token = nn.Parameter(torch.zeros(1, 1, self.hidden_dim))\n",
    "        def forward(self, x):\n",
    "            n = x.shape[0]\n",
    "            patched = self.patch(x)\n",
    "            batched_cls_token = self.cls_token.expand(n, -1, -1)\n",
    "            patched = torch.cat([batched_cls_token, patched], dim=1)\n",
    "            patched = self.pos_embedding(patched)\n",
    "            patched = self.encoder(patched)\n",
    "            patched = self.layernorm(patched.last_hidden_state)\n",
    "            patched = patched[:, 0]\n",
    "            patched = self.classifier(patched)\n",
    "            return patched\n",
    "\n",
    "    model = ViT_SinCos2D()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CPEViT**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-26T18:59:12.140403Z",
     "iopub.status.busy": "2026-01-26T18:59:12.140135Z",
     "iopub.status.idle": "2026-01-26T18:59:12.161079Z",
     "shell.execute_reply": "2026-01-26T18:59:12.160163Z",
     "shell.execute_reply.started": "2026-01-26T18:59:12.140366Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import ViTModel\n",
    "\n",
    "class ConditionalPositionalEncoding(nn.Module):\n",
    "    def __init__(self, embed_dim, num_patches):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(embed_dim, embed_dim, kernel_size=3, padding=1, groups=embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, N, D = x.shape  # (Batch, Num Tokens, Embedding Dim)\n",
    "        H = W = int((N - 1) ** 0.5)  # Ignore CLS token for reshaping\n",
    "\n",
    "        cls_token = x[:, 0:1, :]  # Extract CLS token\n",
    "        x = x[:, 1:, :]\n",
    "        \n",
    "        # Reshape for convolution\n",
    "        cpe = x.transpose(1, 2).reshape(B, D, H, W)  # (B, D, H, W)\n",
    "        cpe = self.conv(cpe).flatten(2).transpose(1, 2)  # Apply CPE\n",
    "\n",
    "        x = x + cpe  # Add CPE to the features\n",
    "\n",
    "        return torch.cat([cls_token, x], dim=1)  # Reinsert CLS token\n",
    "\n",
    "class CPEViT(nn.Module):\n",
    "    def __init__(self, model_name=\"google/vit-base-patch16-224\"):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.vit = ViTModel.from_pretrained(model_name)\n",
    "        self.vit.embeddings.position_embeddings = None  # Remove default positional embeddings\n",
    "\n",
    "        embed_dim = self.vit.config.hidden_size\n",
    "        num_patches = (self.vit.config.image_size // self.vit.config.patch_size) ** 2  # Compute patches\n",
    "\n",
    "        self.encoder1 = self.vit.encoder.layer[0]\n",
    "        self.cpe = ConditionalPositionalEncoding(embed_dim, num_patches)\n",
    "        self.remaining_encoders = nn.ModuleList(self.vit.encoder.layer[1:])\n",
    "        self.fc = nn.Linear(embed_dim, __NUM_CLASS__)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.vit.embeddings.patch_embeddings(x)  # (B, 196, D)\n",
    "\n",
    "        batch_size = x.shape[0]\n",
    "        cls_tokens = self.vit.embeddings.cls_token.expand(batch_size, -1, -1)  # (B, 1, D)\n",
    "        x = torch.cat([cls_tokens, x], dim=1)  # (B, 197, D)\n",
    "\n",
    "        x = self.encoder1(x)[0]  # Extract tensor from tuple\n",
    "\n",
    "        x = self.cpe(x)  # Apply Conditional Positional Encoding\n",
    "\n",
    "        for layer in self.remaining_encoders:\n",
    "            x = layer(x)[0]  # Extract tensor from tuple\n",
    "\n",
    "        cls_token_final = x[:, 0]  # (B, D) - Extract the class token's final representation\n",
    "        out = self.fc(cls_token_final)  # (B, num_classes) - Classification output\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-26T18:59:12.162497Z",
     "iopub.status.busy": "2026-01-26T18:59:12.162206Z",
     "iopub.status.idle": "2026-01-26T18:59:12.178737Z",
     "shell.execute_reply": "2026-01-26T18:59:12.177947Z",
     "shell.execute_reply.started": "2026-01-26T18:59:12.162468Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "if __MODEL__ == 'CPEViT' or __MODEL__ == 'pretest':\n",
    "    model = CPEViT(\"google/vit-base-patch16-224\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**RPE ViT**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-26T18:59:12.180341Z",
     "iopub.status.busy": "2026-01-26T18:59:12.179915Z",
     "iopub.status.idle": "2026-01-26T18:59:12.197547Z",
     "shell.execute_reply": "2026-01-26T18:59:12.196577Z",
     "shell.execute_reply.started": "2026-01-26T18:59:12.180312Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "if __MODEL__ == 'RPE' or __MODEL__ == 'pretest':    \n",
    "    \n",
    "    # Load Pretrained ViT Model\n",
    "    model = ViTForImageClassification.from_pretrained('google/vit-base-patch16-224')\n",
    "    _, seq_length, hidden_dims = model.vit.embeddings.position_embeddings.shape\n",
    "    model.vit.embeddings.position_embeddings = pe_caller(hidden_dims, seq_length)\n",
    "    model.classifier = nn.Linear(in_features=768, out_features=__NUM_CLASS__, bias=True)\n",
    "    \n",
    "    # Compute grid size dynamically\n",
    "    grid_size = model.config.image_size // model.config.patch_size  # Example: 224//16 = 14\n",
    "    \n",
    "    class GlobalRelativePositionBias(nn.Module):\n",
    "        def __init__(self, grid_size, num_heads):\n",
    "            super().__init__()\n",
    "            self.grid_size = grid_size\n",
    "            self.num_heads = num_heads\n",
    "    \n",
    "            # Define a learnable relative position bias table\n",
    "            self.relative_position_bias_table = nn.Parameter(\n",
    "                torch.zeros((2 * grid_size - 1) * (2 * grid_size - 1), num_heads)\n",
    "            )  # Shape: ((2H-1) * (2W-1), nH)\n",
    "    \n",
    "            # Initialize with truncated normal\n",
    "            nn.init.trunc_normal_(self.relative_position_bias_table, std=0.02)\n",
    "    \n",
    "            # Compute relative position indices\n",
    "            self.define_relative_position_index()\n",
    "    \n",
    "        def define_relative_position_index(self):\n",
    "            \"\"\"Compute pairwise relative position indices for the full grid.\"\"\"\n",
    "            coords_h = torch.arange(self.grid_size)\n",
    "            coords_w = torch.arange(self.grid_size)\n",
    "            coords = torch.stack(torch.meshgrid(coords_h, coords_w, indexing=\"ij\"))  # Shape: (2, H, W)\n",
    "            #print(coords)\n",
    "    \n",
    "            \n",
    "    \n",
    "            coords_flatten = torch.flatten(coords, 1)  # Shape: (2, H*W)\n",
    "            #print(coords_flatten.shape)\n",
    "            relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]  # Shape: (2, H*W, H*W)\n",
    "    \n",
    "            #print(coords_flatten[:, None,:].shape)\n",
    "    \n",
    "            #print(relative_coords[0,0,:])\n",
    "    \n",
    "            relative_coords = relative_coords.permute(1, 2, 0).contiguous()  # Shape: (H*W, H*W, 2)\n",
    "            relative_coords[:, :, 0] += self.grid_size - 1  # Shift to start from 0\n",
    "            relative_coords[:, :, 1] += self.grid_size - 1\n",
    "    \n",
    "            relative_coords[:, :, 0] *= 2 * self.grid_size - 1\n",
    "            relative_position_index = relative_coords.sum(-1).flatten()  # Shape: (H*W * H*W)\n",
    "    \n",
    "            self.register_buffer(\"relative_position_index\", relative_position_index)\n",
    "    \n",
    "        def forward(self):\n",
    "            \"\"\"Retrieve relative position bias.\"\"\"\n",
    "            return self.relative_position_bias_table[self.relative_position_index].view(\n",
    "                self.grid_size * self.grid_size, self.grid_size * self.grid_size, self.num_heads\n",
    "            ).permute(2, 0, 1)  # Shape: (nH, H*W, H*W)\n",
    "    \n",
    "    \n",
    "    class RifatAttention(nn.Module):\n",
    "        def __init__(self, pretrained_query, pretrained_key, pretrained_value, grid_size, num_heads):\n",
    "            super().__init__()\n",
    "            self.query_ = nn.Parameter(pretrained_query.weight)\n",
    "            self.key_ = nn.Parameter(pretrained_key.weight)\n",
    "            self.value_ = nn.Parameter(pretrained_value.weight)\n",
    "    \n",
    "            self.relative_position_bias_module = GlobalRelativePositionBias(grid_size, num_heads)\n",
    "    \n",
    "        def forward(self, hidden_states, attention_mask=None, head_mask=None, output_attentions=False):\n",
    "            q = torch.matmul(hidden_states, self.query_.T)\n",
    "            k = torch.matmul(hidden_states, self.key_.T)\n",
    "            v = torch.matmul(hidden_states, self.value_.T)\n",
    "    \n",
    "            print(f'query shape:{q.shape}')\n",
    "            print(f'key shape:{k.shape}')\n",
    "    \n",
    "            attn_scores = torch.matmul(q, k.transpose(-2,-1))  # Standard attention\n",
    "            print(f'attention score shape:{attn_scores.shape}')\n",
    "    \n",
    "            # Add relative positional bias (broadcasted across batches)\n",
    "            #attn_scores += self.relative_position_bias_module().unsqueeze(0)\n",
    "            print(f' RPE shape: {self.relative_position_bias_module().shape}')\n",
    "            print(attn_scores[:, 1: , 1: ].shape)\n",
    "            attn_scores[:, 1: , 1: ] += self.relative_position_bias_module()\n",
    "            print(f'attention score after RPE shape:{attn_scores.shape}')\n",
    "    \n",
    "    \n",
    "            # Apply mask if available\n",
    "            if attention_mask is not None:\n",
    "                attn_scores = attn_scores + attention_mask  # Hugging Face applies -10000 for padding tokens\n",
    "    \n",
    "            attn_probs = torch.softmax(attn_scores, dim=-1)\n",
    "    \n",
    "            # Apply head mask if given\n",
    "            if head_mask is not None:\n",
    "                attn_probs = attn_probs * head_mask\n",
    "    \n",
    "            x = torch.matmul(attn_probs, v)\n",
    "    \n",
    "            if output_attentions:\n",
    "                return x, attn_probs\n",
    "            return x\n",
    "    \n",
    "    for i in range(12):\n",
    "        for name, module in model.vit.encoder.layer[i].named_modules():\n",
    "            if 'attention.attention' in name:\n",
    "                query = model.vit.encoder.layer[i].attention.attention.query\n",
    "                key = model.vit.encoder.layer[i].attention.attention.key\n",
    "                value = model.vit.encoder.layer[i].attention.attention.value\n",
    "                module = RifatAttention(\n",
    "                query, key, value, grid_size, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ViT_B_16**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-26T18:59:12.198950Z",
     "iopub.status.busy": "2026-01-26T18:59:12.198634Z",
     "iopub.status.idle": "2026-01-26T18:59:13.505557Z",
     "shell.execute_reply": "2026-01-26T18:59:13.504605Z",
     "shell.execute_reply.started": "2026-01-26T18:59:12.198912Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from transformers import ViTConfig, ViTForImageClassification\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "if __MODEL__ == 'vit_b_16' or __MODEL__ == 'pretest':\n",
    "\n",
    "    config = ViTConfig(\n",
    "        image_size=__IMG_SIZE__,          # 224\n",
    "        patch_size=16,\n",
    "        num_channels=3,\n",
    "        hidden_size=768,\n",
    "        num_hidden_layers=12,\n",
    "        num_attention_heads=12,\n",
    "        intermediate_size=3072,\n",
    "\n",
    "        num_labels=__NUM_CLASS__,\n",
    "\n",
    "        # DeiT-style: usually keep dropout ~0 and use drop-path + strong aug + mixup/cutmix\n",
    "        hidden_dropout_prob=0.0,\n",
    "        attention_probs_dropout_prob=0.0,\n",
    "    )\n",
    "\n",
    "    # stochastic depth (drop-path)\n",
    "    if hasattr(config, \"drop_path_rate\"):\n",
    "        config.drop_path_rate = 0.1\n",
    "    else:\n",
    "        # older transformers: safe to skip, training still works\n",
    "        pass\n",
    "\n",
    "    # scratch init\n",
    "    model = ViTForImageClassification(config)\n",
    "\n",
    "    # replace position embeddings (robust to tensor vs Parameter)\n",
    "    _, seq_length, hidden_dims = model.vit.embeddings.position_embeddings.shape\n",
    "    new_pe = pe_caller(hidden_dims, seq_length)\n",
    "\n",
    "    if isinstance(new_pe, torch.Tensor) and not isinstance(new_pe, nn.Parameter):\n",
    "        new_pe = nn.Parameter(new_pe)\n",
    "\n",
    "    model.vit.embeddings.position_embeddings = new_pe\n",
    "\n",
    "    # classifier is already correct because num_labels was set\n",
    "    # model.classifier = nn.Linear(768, __NUM_CLASS__, bias=True)  # not needed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DEIT**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-26T18:59:13.507080Z",
     "iopub.status.busy": "2026-01-26T18:59:13.506696Z",
     "iopub.status.idle": "2026-01-26T18:59:13.512742Z",
     "shell.execute_reply": "2026-01-26T18:59:13.512037Z",
     "shell.execute_reply.started": "2026-01-26T18:59:13.507018Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "if __MODEL__ == 'deit_b_16' or __MODEL__ == 'pretest':\n",
    "    model = ViTForImageClassification.from_pretrained('facebook/deit-base-patch16-224')\n",
    "    _, seq_length, hidden_dim = model.vit.embeddings.position_embeddings.shape\n",
    "    model.vit.embeddings.position_embeddings = pe_caller(hidden_dim, seq_length)\n",
    "    model.classifier = nn.Linear(in_features=768, out_features=__NUM_CLASS__, bias=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**BEIT**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-26T18:59:13.514154Z",
     "iopub.status.busy": "2026-01-26T18:59:13.513866Z",
     "iopub.status.idle": "2026-01-26T18:59:13.527821Z",
     "shell.execute_reply": "2026-01-26T18:59:13.526998Z",
     "shell.execute_reply.started": "2026-01-26T18:59:13.514129Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "if __MODEL__ == 'beit_b_16' or __MODEL__=='pretest':\n",
    "    model = BeitForImageClassification.from_pretrained('microsoft/beit-base-patch16-224')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cross-ViT**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-26T18:59:13.531995Z",
     "iopub.status.busy": "2026-01-26T18:59:13.531624Z",
     "iopub.status.idle": "2026-01-26T18:59:13.542908Z",
     "shell.execute_reply": "2026-01-26T18:59:13.542167Z",
     "shell.execute_reply.started": "2026-01-26T18:59:13.531974Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "if __MODEL__ == 'crossvit_base_240' or __MODEL__ == 'pretest' :\n",
    "    model = timm.create_model( 'crossvit_base_240.in1k', pretrained = True)\n",
    "    \n",
    "    _, seq_length, hidden_dim = model.pos_embed_0.shape\n",
    "    model.pos_embed_0 = pe_caller(hidden_dim, seq_length)\n",
    "    \n",
    "    _, seq_length, hidden_dim = model.pos_embed_1.shape\n",
    "    model.pos_embed_1 = pe_caller(hidden_dim, seq_length)\n",
    "    \n",
    "    model.head = nn.ModuleList([\n",
    "            nn.Linear(in_features=head.in_features, out_features=__NUM_CLASS__, bias=True) \n",
    "            for head in model.head])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CaiT**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-26T18:59:13.544496Z",
     "iopub.status.busy": "2026-01-26T18:59:13.543961Z",
     "iopub.status.idle": "2026-01-26T18:59:13.561010Z",
     "shell.execute_reply": "2026-01-26T18:59:13.560291Z",
     "shell.execute_reply.started": "2026-01-26T18:59:13.544468Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "if __MODEL__ == 'cait_s24_224' or __MODEL__ == 'pretest':\n",
    "    model = timm.create_model('cait_s24_224.fb_dist_in1k', pretrained=True)\n",
    "\n",
    "    # Get model's embedding dimensions\n",
    "    _, seq_length, hidden_dim = model.pos_embed.shape\n",
    "\n",
    "    # Replace positional embeddings\n",
    "    temp = pe_caller(hidden_dim, seq_length+1)\n",
    "    model.pos_embed = nn.Parameter(temp[:,1:,:],requires_grad=temp.requires_grad)\n",
    "    model.cls_token = nn.Parameter(temp[:,:1,:],requires_grad=temp.requires_grad)\n",
    "\n",
    "    # Modify classifier layer\n",
    "    model.head = nn.Linear(in_features=hidden_dim, out_features=__NUM_CLASS__, bias=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cross RPE (Useless)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-26T18:59:13.562443Z",
     "iopub.status.busy": "2026-01-26T18:59:13.562047Z",
     "iopub.status.idle": "2026-01-26T18:59:13.582914Z",
     "shell.execute_reply": "2026-01-26T18:59:13.582284Z",
     "shell.execute_reply.started": "2026-01-26T18:59:13.562416Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import ViTModel\n",
    "from transformers.models.vit.modeling_vit import ViTSelfAttention\n",
    "\n",
    "if __MODEL__ == 'Cross-RPE' or __MODEL__ == 'pretest':\n",
    "    class CrossMethodRPE(nn.Module):\n",
    "        def __init__(self, image_size, embed_dim, alpha=1.0, beta=10.0, gamma=100.0):\n",
    "            super(CrossMethodRPE, self).__init__()\n",
    "    \n",
    "            self.image_size = image_size  # (Height, Width)\n",
    "            self.height, self.width = image_size\n",
    "            self.embed_dim = embed_dim\n",
    "    \n",
    "            # Learnable scalars for horizontal and vertical directions\n",
    "            self.px = nn.Parameter(torch.randn(embed_dim, 1, 1))  # Horizontal\n",
    "            self.py = nn.Parameter(torch.randn(embed_dim, 1, 1))  # Vertical\n",
    "    \n",
    "            # Parameters for piecewise function g(x)\n",
    "            self.alpha = alpha\n",
    "            self.beta = beta\n",
    "            self.gamma = gamma\n",
    "    \n",
    "        def g(self, dist):\n",
    "            alpha_tensor = torch.tensor(self.alpha, device=dist.device)\n",
    "            gamma_tensor = torch.tensor(self.gamma, device=dist.device)\n",
    "    \n",
    "            dist = dist.abs()\n",
    "\n",
    "            # Case 1: |x| ≤ α\n",
    "            less_than_alpha = dist <= alpha_tensor\n",
    "            result = torch.round(dist) * less_than_alpha.float()\n",
    "    \n",
    "            # Case 2: |x| > α\n",
    "            greater_than_alpha = dist > alpha_tensor\n",
    "            sign_dist = torch.sign(dist)\n",
    "    \n",
    "            log_scaled = alpha_tensor + (torch.log(dist / alpha_tensor) / torch.log(gamma_tensor / alpha_tensor)) * (self.beta - alpha_tensor)\n",
    "            result += sign_dist * torch.min(self.beta * torch.ones_like(log_scaled), log_scaled) * greater_than_alpha.float()\n",
    "    \n",
    "            return result\n",
    "    \n",
    "        def forward(self):\n",
    "            \"\"\"Computes relative positional encoding b_ij\"\"\"\n",
    "            grid_x, grid_y = torch.meshgrid(torch.arange(self.height), torch.arange(self.width), indexing=\"ij\")\n",
    "            grid_x = grid_x.to(torch.float32).unsqueeze(0).unsqueeze(0)\n",
    "            grid_y = grid_y.to(torch.float32).unsqueeze(0).unsqueeze(0)\n",
    "    \n",
    "            Ix = self.g(grid_x - grid_x.transpose(-1, -2)) * self.px\n",
    "            Iy = self.g(grid_y - grid_y.transpose(-1, -2)) * self.py\n",
    "    \n",
    "            encoding = Ix + Iy  # Final RPE b_ij\n",
    "            return encoding\n",
    "    \n",
    "    \n",
    "    class ViTSelfAttentionWithRPE(ViTSelfAttention):\n",
    "        def __init__(self, config, rpe_module):\n",
    "            super().__init__(config)\n",
    "            self.rpe_module = rpe_module  # Inject RPE module\n",
    "\n",
    "        def forward(self, hidden_states, head_mask=None, output_attentions=False):\n",
    "            batch_size, seq_len, hidden_dim = hidden_states.shape\n",
    "            num_heads = self.num_attention_heads\n",
    "            head_dim = hidden_dim // num_heads\n",
    "    \n",
    "            # Project queries, keys, values\n",
    "            query_layer = self.query(hidden_states).view(batch_size, seq_len, num_heads, head_dim).transpose(1, 2)\n",
    "            key_layer = self.key(hidden_states).view(batch_size, seq_len, num_heads, head_dim).transpose(1, 2)\n",
    "            value_layer = self.value(hidden_states).view(batch_size, seq_len, num_heads, head_dim).transpose(1, 2)\n",
    "    \n",
    "            # Compute attention scores\n",
    "            attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2)) / (head_dim ** 0.5)\n",
    "    \n",
    "            # Add relative positional encoding b_ij\n",
    "            rpe = self.rpe_module().to(hidden_states.device)  # Compute b_ij\n",
    "            rpe = rpe.unsqueeze(0).unsqueeze(0)  # Shape: [1, 1, height, width]\n",
    "            attention_scores = attention_scores + rpe\n",
    "    \n",
    "            # Softmax and apply attention\n",
    "            attention_probs = F.softmax(attention_scores, dim=-1)\n",
    "            attention_probs = self.attention_dropout(attention_probs)\n",
    "    \n",
    "            # Compute context\n",
    "            context_layer = torch.matmul(attention_probs, value_layer)\n",
    "            context_layer = context_layer.transpose(1, 2).reshape(batch_size, seq_len, hidden_dim)\n",
    "    \n",
    "            # Apply output projection\n",
    "            context_layer = self.dense(context_layer)\n",
    "    \n",
    "            # Return values in the expected format\n",
    "            outputs = (context_layer, attention_probs) if output_attentions else (context_layer,)\n",
    "            return outputs\n",
    "    \n",
    "    \n",
    "    model = ViTModel.from_pretrained(\"google/vit-base-patch16-224-in21k\")\n",
    "    \n",
    "    # Extract model configuration\n",
    "    hidden_size = model.config.hidden_size  # dz\n",
    "    image_size = (14, 14)  # ViT input size after patch embedding (224/16 = 14)\n",
    "    rpe_module = CrossMethodRPE(image_size=image_size, embed_dim=hidden_size)\n",
    "    \n",
    "    # Replace the self-attention module in all transformer blocks\n",
    "    for layer in model.encoder.layer:\n",
    "        layer.attention.attention = ViTSelfAttentionWithRPE(model.config, rpe_module)\n",
    "    \n",
    "    print(\"Modified ViT with Cross Method RPE is ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Trainer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-26T18:59:13.584178Z",
     "iopub.status.busy": "2026-01-26T18:59:13.583917Z",
     "iopub.status.idle": "2026-01-26T18:59:13.602669Z",
     "shell.execute_reply": "2026-01-26T18:59:13.601903Z",
     "shell.execute_reply.started": "2026-01-26T18:59:13.584156Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-26T18:59:13.603992Z",
     "iopub.status.busy": "2026-01-26T18:59:13.603620Z",
     "iopub.status.idle": "2026-01-26T18:59:13.622495Z",
     "shell.execute_reply": "2026-01-26T18:59:13.621553Z",
     "shell.execute_reply.started": "2026-01-26T18:59:13.603954Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "def Trainer(train_loader, model, optimizer, epoch, device, grad_accum_steps=1, max_grad_norm=1.0, label_smoothing=0.0):\n",
    "    model.train()\n",
    "\n",
    "    grad_accum_steps = max(1, int(grad_accum_steps))\n",
    "    current_lr = optimizer.param_groups[0][\"lr\"]\n",
    "    print(f\"Epoch [{epoch}] - Learning Rate: {current_lr:.6f}\")\n",
    "\n",
    "    losses = AverageMeter()\n",
    "    bce_crit = nn.BCEWithLogitsLoss()\n",
    "    ce_crit = nn.CrossEntropyLoss(label_smoothing=label_smoothing) if label_smoothing > 0 else nn.CrossEntropyLoss()\n",
    "\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    accum_count = 0\n",
    "    micro_steps = 0\n",
    "    total_batches = len(train_loader)\n",
    "\n",
    "    for idx, (images, labels) in enumerate(train_loader):\n",
    "        images = images.to(device, non_blocking=True)\n",
    "        labels = labels.to(device, non_blocking=True)\n",
    "\n",
    "        out = model(images)\n",
    "        logits = out.logits if hasattr(out, \"logits\") else out\n",
    "\n",
    "        if __DATASET__ == \"new_dataset\":\n",
    "            loss = bce_crit(logits, labels.float())\n",
    "        else:\n",
    "            targets = labels.argmax(dim=1).long() if labels.ndim > 1 else labels.long()\n",
    "            loss = ce_crit(logits, targets)\n",
    "\n",
    "        losses.update(loss.detach().item(), labels.size(0))\n",
    "\n",
    "        accum_count += 1\n",
    "        micro_steps += 1\n",
    "\n",
    "        # scale by actual number of micro-batches in this update\n",
    "        if (idx == total_batches - 1) and (accum_count != grad_accum_steps):\n",
    "            scale = accum_count\n",
    "        else:\n",
    "            scale = grad_accum_steps\n",
    "\n",
    "        (loss / scale).backward()\n",
    "\n",
    "        do_step = (accum_count == grad_accum_steps) or (idx == total_batches - 1)\n",
    "        if do_step:\n",
    "            if max_grad_norm is not None:\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            accum_count = 0\n",
    "\n",
    "        if micro_steps % __STEP_COUNTER__ == 0:\n",
    "            print(f\"micro-step: {micro_steps} | avg loss: {losses.avg:.6f}\")\n",
    "\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    return losses.avg\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Validator**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-26T18:59:13.623749Z",
     "iopub.status.busy": "2026-01-26T18:59:13.623456Z",
     "iopub.status.idle": "2026-01-26T18:59:13.640876Z",
     "shell.execute_reply": "2026-01-26T18:59:13.640219Z",
     "shell.execute_reply.started": "2026-01-26T18:59:13.623726Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def validator_for_new(model, valid_loader, device):\n",
    "    model.eval()\n",
    "    total = 0\n",
    "    dist_comp = 0\n",
    "    orientation = 0\n",
    "    area_comp = 0\n",
    "    vec_sum = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in valid_loader:\n",
    "            images = images.to(device)\n",
    "            outputs = model(images)\n",
    "            try:\n",
    "                outputs = nn.Sigmoid()(outputs.logits).detach().cpu().numpy()\n",
    "            except:\n",
    "                outputs = nn.Sigmoid()(outputs).detach().cpu().numpy()\n",
    "            outputs = np.where(outputs>0.5, 1, 0)\n",
    "            labels = np.where(labels>0.5, 1, 0)\n",
    "            dist_comp += (outputs[:,:2]==labels[:,:2]).sum()\n",
    "            orientation += (outputs[:,2]==labels[:,2]).sum()\n",
    "            area_comp += (outputs[:,3:5]==labels[:,3:5]).sum()\n",
    "            vec_sum += (outputs[:,5]==labels[:,5]).sum()\n",
    "            total += labels.shape[0]\n",
    "    dist_comp_acc = 100*dist_comp/(total*2)\n",
    "    orientation_acc = 100*orientation/(total)\n",
    "    area_comp_acc = 100*area_comp/(2*total)\n",
    "    vec_sum_acc = 100*vec_sum/total\n",
    "    avg_accuracy = (dist_comp_acc + orientation_acc + area_comp_acc + vec_sum_acc)/4\n",
    "    print(f'Distance comparison accuracy: {dist_comp_acc:.4f}%')\n",
    "    print(f'Orientation accuracy: {orientation_acc:.4f}%')\n",
    "    print(f'Area comparison accuracy: {area_comp_acc:.4f}%')\n",
    "    print(f'Vector sum accuracy: {vec_sum_acc:.4f}%')\n",
    "    print(f'Average Accuracy: {avg_accuracy:.4f}%')\n",
    "    return avg_accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-26T18:59:13.641997Z",
     "iopub.status.busy": "2026-01-26T18:59:13.641745Z",
     "iopub.status.idle": "2026-01-26T18:59:13.657491Z",
     "shell.execute_reply": "2026-01-26T18:59:13.656891Z",
     "shell.execute_reply.started": "2026-01-26T18:59:13.641975Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def validator_regular(model, valid_loader, device):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in valid_loader:\n",
    "            images = images.to(device, non_blocking=True)\n",
    "            labels = labels.to(device, non_blocking=True)\n",
    "\n",
    "            out = model(images)\n",
    "            logits = out if torch.is_tensor(out) else out.logits  # [B, C]\n",
    "\n",
    "            preds = logits.argmax(dim=1)  # [B]\n",
    "\n",
    "            # support both one-hot labels [B, C] and index labels [B]\n",
    "            if labels.ndim > 1:\n",
    "                targets = labels.argmax(dim=1)\n",
    "            else:\n",
    "                targets = labels.long()\n",
    "\n",
    "            correct += (preds == targets).sum().item()\n",
    "            total += targets.size(0)\n",
    "\n",
    "    accuracy = 100.0 * correct / max(1, total)\n",
    "    print(f'Validation/Test Accuracy: {accuracy:.4f}%')\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-26T18:59:13.658571Z",
     "iopub.status.busy": "2026-01-26T18:59:13.658300Z",
     "iopub.status.idle": "2026-01-26T18:59:13.674739Z",
     "shell.execute_reply": "2026-01-26T18:59:13.673970Z",
     "shell.execute_reply.started": "2026-01-26T18:59:13.658542Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "if __DATASET__ == 'new_dataset':\n",
    "    validator = validator_for_new\n",
    "else:\n",
    "    validator = validator_regular"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Prepare Hardware**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-26T18:59:13.676420Z",
     "iopub.status.busy": "2026-01-26T18:59:13.676171Z",
     "iopub.status.idle": "2026-01-26T18:59:13.897042Z",
     "shell.execute_reply": "2026-01-26T18:59:13.896310Z",
     "shell.execute_reply.started": "2026-01-26T18:59:13.676400Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEVICE WE WILL BE USING IS  cuda:0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "# ----------------------------\n",
    "# device\n",
    "# ----------------------------\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"DEVICE WE WILL BE USING IS \", device)\n",
    "model = model.to(device)\n",
    "\n",
    "# ----------------------------\n",
    "# DeiT-style: no weight decay on biases/norm + (recommended) cls/pos tokens\n",
    "# ----------------------------\n",
    "def param_groups_weight_decay(model, weight_decay: float = 0.05):\n",
    "    decay, no_decay = [], []\n",
    "    skip_wd_names = (\"position_embeddings\", \"pos_embed\", \"cls_token\", \"dist_token\")\n",
    "    for name, p in model.named_parameters():\n",
    "        if not p.requires_grad:\n",
    "            continue\n",
    "        if (\n",
    "            p.ndim == 1\n",
    "            or name.endswith(\".bias\")\n",
    "            or (\"norm\" in name.lower())\n",
    "            or any(k in name for k in skip_wd_names)\n",
    "        ):\n",
    "            no_decay.append(p)\n",
    "        else:\n",
    "            decay.append(p)\n",
    "\n",
    "    return [\n",
    "        {\"params\": decay, \"weight_decay\": weight_decay},\n",
    "        {\"params\": no_decay, \"weight_decay\": 0.0},\n",
    "    ]\n",
    "\n",
    "# ----------------------------\n",
    "# global batch (make sure these match your actual training loop)\n",
    "# ----------------------------\n",
    "# Prefer using your real variables:\n",
    "# global_batch = __BATCH_SIZE_TRAIN__ * __GRAD_ACCUM_STEPS__ * world_size\n",
    "__BATCH_SIZE__ = 64\n",
    "__GRAD_ACCUM_STEPS__ = 16\n",
    "world_size = 1\n",
    "global_batch = __BATCH_SIZE__ * __GRAD_ACCUM_STEPS__ * world_size\n",
    "\n",
    "# ----------------------------\n",
    "# DeiT-style LR scaling\n",
    "# defaults: lr=5e-4, min_lr=1e-5, warmup_lr=1e-6 at global batch=1024\n",
    "# ----------------------------\n",
    "scale = global_batch / 1024.0\n",
    "__MAX_LR__    = 5e-4 * scale\n",
    "__MIN_LR__    = 1e-5 * scale\n",
    "__WARMUP_LR__ = 1e-6 * scale\n",
    "\n",
    "# Safety: never let min_lr exceed max_lr\n",
    "__MIN_LR__ = min(__MIN_LR__, __MAX_LR__)\n",
    "\n",
    "# ----------------------------\n",
    "# optimizer: AdamW (DeiT default wd=0.05, eps=1e-8)\n",
    "# ----------------------------\n",
    "optimizer = optim.AdamW(\n",
    "    param_groups_weight_decay(model, weight_decay=0.05),\n",
    "    lr=__MAX_LR__,\n",
    "    betas=(0.9, 0.999),\n",
    "    eps=1e-8\n",
    ")\n",
    "\n",
    "# ----------------------------\n",
    "# scheduler: warmup -> cosine (STRICT epoch-based stepping)\n",
    "# call scheduler.step() once per epoch\n",
    "# ----------------------------\n",
    "warmup_epochs = 5\n",
    "total_epochs = __EPOCHS__\n",
    "\n",
    "start_factor = (__WARMUP_LR__ / __MAX_LR__) if __MAX_LR__ > 0 else 1.0\n",
    "start_factor = max(1e-8, min(1.0, start_factor))\n",
    "\n",
    "warmup = optim.lr_scheduler.LinearLR(\n",
    "    optimizer,\n",
    "    start_factor=start_factor,\n",
    "    end_factor=1.0,\n",
    "    total_iters=warmup_epochs\n",
    ")\n",
    "\n",
    "cosine = optim.lr_scheduler.CosineAnnealingLR(\n",
    "    optimizer,\n",
    "    T_max=max(1, total_epochs - warmup_epochs),\n",
    "    eta_min=__MIN_LR__\n",
    ")\n",
    "\n",
    "scheduler = optim.lr_scheduler.SequentialLR(\n",
    "    optimizer,\n",
    "    schedulers=[warmup, cosine],\n",
    "    milestones=[warmup_epochs]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Main Loop**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-26T18:59:13.898107Z",
     "iopub.status.busy": "2026-01-26T18:59:13.897875Z",
     "iopub.status.idle": "2026-01-26T18:59:13.903075Z",
     "shell.execute_reply": "2026-01-26T18:59:13.902254Z",
     "shell.execute_reply.started": "2026-01-26T18:59:13.898086Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# DataLoaders (recommended small fixes: valid/test shuffle=False, workers>0 if possible)\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=__BATCH_SIZE_TRAIN__,\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    "    pin_memory=True\n",
    ")\n",
    "valid_loader = torch.utils.data.DataLoader(\n",
    "    valid_dataset,\n",
    "    batch_size=__BATCH_SIZE_VALID__,\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    "    pin_memory=True\n",
    ")\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=__BATCH_SIZE_VALID__,\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    "    pin_memory=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-26T18:59:13.905008Z",
     "iopub.status.busy": "2026-01-26T18:59:13.904175Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1] - Learning Rate: 0.000001\n",
      "micro-step: 50 | avg loss: 4.748254\n",
      "micro-step: 100 | avg loss: 4.728645\n",
      "micro-step: 150 | avg loss: 4.718540\n",
      "micro-step: 200 | avg loss: 4.702933\n",
      "micro-step: 250 | avg loss: 4.688892\n",
      "micro-step: 300 | avg loss: 4.677930\n",
      "micro-step: 350 | avg loss: 4.668380\n",
      "micro-step: 400 | avg loss: 4.659218\n",
      "micro-step: 450 | avg loss: 4.650622\n",
      "micro-step: 500 | avg loss: 4.642492\n",
      "micro-step: 550 | avg loss: 4.634812\n",
      "micro-step: 600 | avg loss: 4.628419\n",
      "micro-step: 650 | avg loss: 4.621867\n",
      "micro-step: 700 | avg loss: 4.616164\n",
      "micro-step: 750 | avg loss: 4.610176\n",
      "micro-step: 800 | avg loss: 4.605120\n",
      "micro-step: 850 | avg loss: 4.600066\n",
      "micro-step: 900 | avg loss: 4.595452\n",
      "micro-step: 950 | avg loss: 4.591213\n",
      "micro-step: 1000 | avg loss: 4.586459\n",
      "micro-step: 1050 | avg loss: 4.581948\n",
      "train loss for epoch 1: 4.579237996614896\n",
      "validating.....\n",
      "Validation/Test Accuracy: 3.6308%\n",
      "saving best (epoch=1, val_acc=3.6308).....\n",
      "Validation/Test Accuracy: 3.3077%\n",
      "test acc (best checkpoint): 3.3077\n",
      "best so far: epoch=1, val_acc=3.6308\n",
      "time elapsed (sec): 1998.46\n",
      "Epoch [2] - Learning Rate: 0.000101\n",
      "micro-step: 50 | avg loss: 4.544177\n",
      "micro-step: 100 | avg loss: 4.494277\n",
      "micro-step: 150 | avg loss: 4.459684\n",
      "micro-step: 200 | avg loss: 4.433586\n",
      "micro-step: 250 | avg loss: 4.418447\n",
      "micro-step: 300 | avg loss: 4.403691\n",
      "micro-step: 350 | avg loss: 4.389847\n",
      "micro-step: 400 | avg loss: 4.376066\n",
      "micro-step: 450 | avg loss: 4.365251\n",
      "micro-step: 500 | avg loss: 4.354192\n",
      "micro-step: 550 | avg loss: 4.344362\n",
      "micro-step: 600 | avg loss: 4.333315\n",
      "micro-step: 650 | avg loss: 4.321751\n",
      "micro-step: 700 | avg loss: 4.310341\n",
      "micro-step: 750 | avg loss: 4.295920\n",
      "micro-step: 800 | avg loss: 4.283335\n",
      "micro-step: 850 | avg loss: 4.270146\n",
      "micro-step: 900 | avg loss: 4.257731\n",
      "micro-step: 950 | avg loss: 4.244172\n",
      "micro-step: 1000 | avg loss: 4.230938\n",
      "micro-step: 1050 | avg loss: 4.218534\n",
      "train loss for epoch 2: 4.210513563963083\n",
      "validating.....\n",
      "Validation/Test Accuracy: 12.4385%\n",
      "saving best (epoch=2, val_acc=12.4385).....\n",
      "Validation/Test Accuracy: 12.7538%\n",
      "test acc (best checkpoint): 12.7538\n",
      "best so far: epoch=2, val_acc=12.4385\n",
      "time elapsed (sec): 1995.77\n",
      "Epoch [3] - Learning Rate: 0.000201\n",
      "micro-step: 50 | avg loss: 3.948280\n",
      "micro-step: 100 | avg loss: 3.926984\n",
      "micro-step: 150 | avg loss: 3.904063\n",
      "micro-step: 200 | avg loss: 3.888120\n",
      "micro-step: 250 | avg loss: 3.876581\n",
      "micro-step: 300 | avg loss: 3.861456\n",
      "micro-step: 350 | avg loss: 3.853505\n",
      "micro-step: 400 | avg loss: 3.836811\n",
      "micro-step: 450 | avg loss: 3.823304\n",
      "micro-step: 500 | avg loss: 3.810964\n",
      "micro-step: 550 | avg loss: 3.803056\n",
      "micro-step: 600 | avg loss: 3.790747\n",
      "micro-step: 650 | avg loss: 3.779885\n",
      "micro-step: 700 | avg loss: 3.769122\n",
      "micro-step: 750 | avg loss: 3.760398\n",
      "micro-step: 800 | avg loss: 3.750176\n",
      "micro-step: 850 | avg loss: 3.741269\n",
      "micro-step: 900 | avg loss: 3.731408\n",
      "micro-step: 950 | avg loss: 3.721973\n",
      "micro-step: 1000 | avg loss: 3.713046\n",
      "micro-step: 1050 | avg loss: 3.705116\n",
      "train loss for epoch 3: 3.69871146436838\n",
      "validating.....\n",
      "Validation/Test Accuracy: 21.1538%\n",
      "saving best (epoch=3, val_acc=21.1538).....\n",
      "Validation/Test Accuracy: 21.4846%\n",
      "test acc (best checkpoint): 21.4846\n",
      "best so far: epoch=3, val_acc=21.1538\n",
      "time elapsed (sec): 1995.96\n",
      "Epoch [4] - Learning Rate: 0.000300\n",
      "micro-step: 50 | avg loss: 3.509248\n",
      "micro-step: 100 | avg loss: 3.545120\n",
      "micro-step: 150 | avg loss: 3.553346\n",
      "micro-step: 200 | avg loss: 3.541449\n",
      "micro-step: 250 | avg loss: 3.531614\n",
      "micro-step: 300 | avg loss: 3.525781\n",
      "micro-step: 350 | avg loss: 3.521100\n",
      "micro-step: 400 | avg loss: 3.511044\n",
      "micro-step: 450 | avg loss: 3.504084\n",
      "micro-step: 500 | avg loss: 3.499930\n",
      "micro-step: 550 | avg loss: 3.494487\n",
      "micro-step: 600 | avg loss: 3.489372\n",
      "micro-step: 650 | avg loss: 3.483500\n",
      "micro-step: 700 | avg loss: 3.477947\n",
      "micro-step: 750 | avg loss: 3.469189\n",
      "micro-step: 800 | avg loss: 3.462683\n",
      "micro-step: 850 | avg loss: 3.453461\n",
      "micro-step: 900 | avg loss: 3.448571\n",
      "micro-step: 950 | avg loss: 3.443199\n",
      "micro-step: 1000 | avg loss: 3.437559\n",
      "micro-step: 1050 | avg loss: 3.431446\n",
      "train loss for epoch 4: 3.4262478496844953\n",
      "validating.....\n",
      "Validation/Test Accuracy: 27.2077%\n",
      "saving best (epoch=4, val_acc=27.2077).....\n",
      "Validation/Test Accuracy: 27.1615%\n",
      "test acc (best checkpoint): 27.1615\n",
      "best so far: epoch=4, val_acc=27.2077\n",
      "time elapsed (sec): 1998.49\n",
      "Epoch [5] - Learning Rate: 0.000400\n",
      "micro-step: 50 | avg loss: 3.219961\n",
      "micro-step: 100 | avg loss: 3.241076\n",
      "micro-step: 150 | avg loss: 3.275138\n",
      "micro-step: 200 | avg loss: 3.278575\n",
      "micro-step: 250 | avg loss: 3.287902\n",
      "micro-step: 300 | avg loss: 3.280847\n",
      "micro-step: 350 | avg loss: 3.281143\n",
      "micro-step: 400 | avg loss: 3.278499\n",
      "micro-step: 450 | avg loss: 3.271106\n",
      "micro-step: 500 | avg loss: 3.262251\n",
      "micro-step: 550 | avg loss: 3.261253\n",
      "micro-step: 600 | avg loss: 3.258118\n",
      "micro-step: 650 | avg loss: 3.253585\n",
      "micro-step: 700 | avg loss: 3.250833\n",
      "micro-step: 750 | avg loss: 3.251130\n"
     ]
    }
   ],
   "source": [
    "best_acc = 0.0\n",
    "best_epoch = START_EPOCH\n",
    "\n",
    "train_losses = []\n",
    "val_accs = []\n",
    "\n",
    "for epoch in range(START_EPOCH, __EPOCHS__ + 1):\n",
    "    start = time.time()\n",
    "\n",
    "    train_loss = Trainer(\n",
    "        train_loader=train_loader,\n",
    "        model=model,\n",
    "        optimizer=optimizer,\n",
    "        epoch=epoch,\n",
    "        device=device,\n",
    "        grad_accum_steps=__GRAD_ACCUM_STEPS__,\n",
    "        max_grad_norm=1.0,     # good default for ViT scratch\n",
    "        # label_smoothing=0.1   # enable only if your Trainer supports it and before mixup/cutmix\n",
    "    )\n",
    "\n",
    "    print(f\"train loss for epoch {epoch}: {train_loss}\")\n",
    "    train_losses.append(train_loss)\n",
    "\n",
    "    print(\"validating.....\")\n",
    "    val_acc = validator(model, valid_loader, device)\n",
    "    val_accs.append(val_acc)\n",
    "\n",
    "    if val_acc > best_acc:\n",
    "        best_acc = val_acc\n",
    "        best_epoch = epoch\n",
    "        print(f\"saving best (epoch={epoch}, val_acc={val_acc:.4f}).....\")\n",
    "        torch.save(model.state_dict(), BASE_PATH)\n",
    "\n",
    "        test_acc = validator(model, test_loader, device)\n",
    "        print(f\"test acc (best checkpoint): {test_acc:.4f}\")\n",
    "\n",
    "    torch.save(model.state_dict(), LAST_PATH)\n",
    "\n",
    "    # STRICT epoch-based scheduler step (exactly once per epoch)\n",
    "    scheduler.step()\n",
    "\n",
    "    end = time.time()\n",
    "    print(f\"best so far: epoch={best_epoch}, val_acc={best_acc:.4f}\")\n",
    "    print(f\"time elapsed (sec): {(end - start):.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "img = valid_dataset[3][0].unsqueeze(0).to(device)\n",
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "pos = model.pos_embedding(img).squeeze().detach().cpu().numpy()[1:].reshape(14,14,768)\n",
    "final = pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "indices = np.arccos(final[:,:,767])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from numpy.linalg import norm\n",
    "cen = (0,0)\n",
    "similarity = ((final * final[cen]).sum(-1))/(norm(final, axis=-1)*norm(final[cen]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "plt.imshow(similarity, cmap='hot')\n",
    "#plt.colorbar()\n",
    "plt.title('14x14 Array Scaled to 224x224',)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def radial_smooth(array, center=None, bin_width=1.0):\n",
    "    nrows, ncols = array.shape\n",
    "    if center is None: center = (nrows / 2, ncols / 2)\n",
    "    a, b = center\n",
    "    y, x = np.indices((nrows, ncols))\n",
    "    r = np.sqrt((y - a)**2 + (x - b)**2)\n",
    "    bin_indices = np.floor(r / bin_width).astype(int)\n",
    "    smoothed = np.zeros_like(array)\n",
    "    for bin_val in np.unique(bin_indices):\n",
    "        mask = (bin_indices == bin_val)\n",
    "        smoothed[mask] = array[mask].mean()\n",
    "    return smoothed\n",
    "\n",
    "smoothed_array = radial_smooth(similarity, center=(0,0), bin_width=1.0)\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(similarity, cmap='viridis')\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(smoothed_array, cmap='viridis')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.ndimage import zoom\n",
    "\n",
    "def radial_smooth(array, center=None, bin_width=1.0):\n",
    "    nrows, ncols = array.shape\n",
    "    if center is None: center = (nrows / 2, ncols / 2)\n",
    "    a, b = center\n",
    "    y, x = np.indices((nrows, ncols))\n",
    "    r = np.sqrt((y - a)**2 + (x - b)**2)\n",
    "    bin_indices = np.floor(r / bin_width).astype(int)\n",
    "    values = np.zeros(np.unique(bin_indices).shape[0])\n",
    "    smoothed = np.zeros_like(array)\n",
    "    for bin_val in np.unique(bin_indices):\n",
    "        mask = (bin_indices == bin_val)\n",
    "        values[bin_val] = array[mask].std()\n",
    "        smoothed[mask] = array[mask].mean()\n",
    "    return smoothed, values\n",
    "\n",
    "array = similarity\n",
    "#array_224 = zoom(array, (224 / 14, 224 / 14), order=1)\n",
    "smoothed_array, values = radial_smooth(array, center=(0,0), bin_width=1.0)\n",
    "\n",
    "plt.figure(figsize=(12,12))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(array, cmap='hot')\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(smoothed_array, cmap='hot')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def running_mean(x, N):\n",
    "    cumsum = np.cumsum(np.insert(x, 0, 0)) \n",
    "    return (cumsum[N:] - cumsum[:-N]) / float(N)\n",
    "plt.plot(running_mean(values,3))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Test Results**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#model.load_state_dict(torch.load(BASE_PATH, weights_only=True))\n",
    "test_acc = validator(model, test_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "img = x.squeeze().cpu().permute(1, 2, 0).numpy()\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.imshow(img)\n",
    "cell_size = 224 // 14\n",
    "\n",
    "for i in range(15):\n",
    "    ax.axhline(i * cell_size, color='white', linewidth=0.5)\n",
    "    ax.axvline(i * cell_size, color='white', linewidth=0.5)\n",
    "\n",
    "ax.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "np.set_printoptions(linewidth=np.inf)\n",
    "\n",
    "x = valid_dataset[3][0].unsqueeze(0).to(device)\n",
    "cord = model.pos_embedding.cord\n",
    "art_conv = model.pos_embedding.art_conv\n",
    "out = art_conv(torch.cat([x, cord.unsqueeze(0).expand(1, -1, -1, -1)], dim=1)).squeeze().detach().cpu().numpy()\n",
    "out = 2*out-1\n",
    "wrap_pos_hilbert = model.pos_embedding.wrap_pos_hilbert.squeeze().detach().cpu().numpy()\n",
    "np.set_printoptions(precision=2, suppress=True)\n",
    "print(out.reshape(14,14))\n",
    "out = out + wrap_pos_hilbert\n",
    "print(wrap_pos_hilbert.reshape(14,14))\n",
    "print(out.reshape(14,14))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Loss & Val_Acc**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "x = np.arange(0, val_accs.__len__(), 1)  # x-axis values  \n",
    "plt.figure(figsize=(8, 5))  # Optional: Set the figure size  \n",
    "plt.plot(x, train_losses, label=\"train_loss\", color=\"blue\", linestyle=\"-\")  # First line  \n",
    "plt.plot(x, val_accs, label=\"val_accuracy\", color=\"red\", linestyle=\"--\")  # Second line  \n",
    "plt.legend()  # Show the legend  \n",
    "plt.grid(True)  # Optional: Add a grid  \n",
    "plt.show()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "p = np.arange(196).reshape((14,14))\n",
    "#p = gilbert_2d(14,14)\n",
    "cen = (6,6)\n",
    "p = p-p[cen]\n",
    "p = (p-p.min())\n",
    "p = p/p.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.ndimage import zoom\n",
    "\n",
    "def radial_smooth(array, center=None, bin_width=1.0):\n",
    "    nrows, ncols = array.shape\n",
    "    if center is None: center = (nrows / 2, ncols / 2)\n",
    "    a, b = center\n",
    "    y, x = np.indices((nrows, ncols))\n",
    "    r = np.sqrt((y - a)**2 + (x - b)**2)\n",
    "    bin_indices = np.floor(r / bin_width).astype(int)\n",
    "    values = np.zeros(np.unique(bin_indices).shape[0])\n",
    "    smoothed = np.zeros_like(array)\n",
    "    for bin_val in np.unique(bin_indices):\n",
    "        mask = (bin_indices == bin_val)\n",
    "        values[bin_val] = array[mask].mean()\n",
    "        smoothed[mask] = array[mask].mean()\n",
    "    return smoothed, values\n",
    "\n",
    "array = p\n",
    "array_224 = zoom(array, (224 / 14, 224 / 14), order=1)\n",
    "smoothed_array, values = radial_smooth(array_224, center=(cen[0]*16+8, cen[1]*16+8), bin_width=1.0)\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(array_224, cmap='viridis')\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(smoothed_array, cmap='viridis')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "plt.plot(values)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "values"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 112480,
     "sourceId": 268736,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 139380,
     "sourceId": 328593,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 1059701,
     "sourceId": 1782442,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 2334639,
     "sourceId": 3932654,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6656324,
     "sourceId": 10735331,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6737647,
     "sourceId": 10848704,
     "sourceType": "datasetVersion"
    }
   ],
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
